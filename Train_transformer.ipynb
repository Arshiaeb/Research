{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pickle\n",
    "from torch.nn.functional import pad, one_hot\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import Transformer\n",
    "from Transformer import TransformerClassifier, MyDataset, create_padding_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine smaller files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ins = []\n",
    "# for i in range (1,16):\n",
    "#     name = \"Train_Data_50_\" +str(i)\n",
    "#     with open(name,\"rb\") as file:\n",
    "#         ins.append(pickle.load(file))\n",
    "\n",
    "# out = pd.concat([x for x in ins])\n",
    "\n",
    "# with open(\"Train_Data_m\",\"wb\") as file:\n",
    "#         pickle.dump(out,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name = \"Train_Data_m\"\n",
    "# with open(name,\"rb\") as file:\n",
    "#     raw_data = pickle.load(file)\n",
    "\n",
    "# raw_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Data\n",
    "name = \"Train_Data_m\"\n",
    "with open(name,\"rb\") as file:\n",
    "    raw_data = pickle.load(file)\n",
    "\n",
    "\n",
    "# Pad Data\n",
    "data = raw_data[\"System_EWS\"]\n",
    "seq = [torch.from_numpy(run).float() for run in data]\n",
    "#seq_trun = [torch.from_numpy(run[:,:6]).float() for run in data]\n",
    "seq_padded = []\n",
    "max_length = 650\n",
    "\n",
    "for run in seq:\n",
    "    pad_amount = max_length - run.shape[0]\n",
    "    run_padded = pad(run, (0, 0, pad_amount, 0))\n",
    "    seq_padded.append(run_padded)\n",
    "\n",
    "input_data = torch.stack(seq_padded)\n",
    "\n",
    "# Create one-hot labels\n",
    "labels = [torch.tensor(label) for label in raw_data[\"null\"]]\n",
    "labels_t = torch.stack(labels)\n",
    "labels_oh = one_hot(labels_t).float()\n",
    "\n",
    "\n",
    "dataset = MyDataset(input_data, labels_oh)\n",
    "batch_size = int(len(raw_data)/45)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([900, 650, 12])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize Model and Select Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_feedforward = 64\n",
    "input_dim = 12  # Number of features\n",
    "d_model = 64  # Transformer model dimension\n",
    "num_heads = 4   # Number of attention heads\n",
    "num_layers = 6  # Number of transformer layers\n",
    "num_classes = 2  # Number of classes for classification\n",
    "dropout = 0.15   # Dropout rate\n",
    "\n",
    "model = TransformerClassifier(input_dim, d_model, num_heads, num_layers, num_classes, dim_feedforward, dropout)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerClassifier(\n",
      "  (batch_norm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (projection): Linear(in_features=12, out_features=64, bias=True)\n",
      "  (pos_encoder): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.15, inplace=False)\n",
      "  )\n",
      "  (transformer_encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-5): 6 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (dropout): Dropout(p=0.15, inplace=False)\n",
      "        (linear2): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.15, inplace=False)\n",
      "        (dropout2): Dropout(p=0.15, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classification_head): Linear(in_features=64, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.7098137140274048\n",
      "Loss: 0.8039897084236145\n",
      "Loss: 1.2751307487487793\n",
      "Loss: 0.9281856417655945\n",
      "Loss: 0.7069808840751648\n",
      "Loss: 0.7660684585571289\n",
      "Loss: 0.5746616721153259\n",
      "Loss: 0.5057948231697083\n",
      "Loss: 0.43982839584350586\n",
      "Loss: 0.6874915957450867\n",
      "Loss: 0.6175878643989563\n",
      "Loss: 0.5559170842170715\n",
      "Loss: 0.49956855177879333\n",
      "Loss: 0.4539553225040436\n",
      "Loss: 0.5956739187240601\n",
      "Loss: 0.4947638511657715\n",
      "Loss: 0.34435293078422546\n",
      "Loss: 0.4547487795352936\n",
      "Loss: 0.41061314940452576\n",
      "Loss: 0.4669405519962311\n",
      "Loss: 0.38921618461608887\n",
      "Loss: 0.3318195939064026\n",
      "Loss: 0.3657333254814148\n",
      "Loss: 0.39640748500823975\n",
      "Loss: 0.5566005706787109\n",
      "Loss: 0.5884007215499878\n",
      "Loss: 0.41651278734207153\n",
      "Loss: 0.44786104559898376\n",
      "Loss: 0.47292953729629517\n",
      "Loss: 0.3076580762863159\n",
      "Loss: 0.51820969581604\n",
      "Loss: 0.5016682147979736\n",
      "Loss: 0.45421963930130005\n",
      "Loss: 0.3093903660774231\n",
      "Loss: 0.41842183470726013\n",
      "Loss: 0.42278680205345154\n",
      "Loss: 0.4157359004020691\n",
      "Loss: 0.21435561776161194\n",
      "Loss: 0.4547498822212219\n",
      "Loss: 0.3808824121952057\n",
      "Loss: 0.44806885719299316\n",
      "Loss: 0.4577764868736267\n",
      "Loss: 0.33364444971084595\n",
      "Loss: 0.4065329432487488\n",
      "Loss: 0.5658247470855713\n",
      "Loss: 0.458381712436676\n",
      "Loss: 0.2434297502040863\n",
      "Loss: 0.3681744933128357\n",
      "Loss: 0.36098814010620117\n",
      "Loss: 0.446469247341156\n",
      "Loss: 0.32435187697410583\n",
      "Loss: 0.3810018002986908\n",
      "Loss: 0.4601825177669525\n",
      "Loss: 0.5594191551208496\n",
      "Loss: 0.3852565884590149\n",
      "Loss: 0.3977263271808624\n",
      "Loss: 0.6211777925491333\n",
      "Loss: 0.40195003151893616\n",
      "Loss: 0.5122684240341187\n",
      "Loss: 0.4201945662498474\n",
      "Loss: 0.47724658250808716\n",
      "Loss: 0.45198336243629456\n",
      "Loss: 0.36570507287979126\n",
      "Loss: 0.413297176361084\n",
      "Loss: 0.4374386668205261\n",
      "Loss: 0.6704267263412476\n",
      "Loss: 0.42782673239707947\n",
      "Loss: 0.30722683668136597\n",
      "Loss: 0.3745307922363281\n",
      "Loss: 0.3228137791156769\n",
      "Loss: 0.4873533248901367\n",
      "Loss: 0.37904006242752075\n",
      "Loss: 0.38825637102127075\n",
      "Loss: 0.3351248502731323\n",
      "Loss: 0.33377236127853394\n",
      "Loss: 0.5133755803108215\n",
      "Loss: 0.3834195137023926\n",
      "Loss: 0.4182659983634949\n",
      "Loss: 0.34271037578582764\n",
      "Loss: 0.3455493748188019\n",
      "Loss: 0.483576238155365\n",
      "Loss: 0.47994518280029297\n",
      "Loss: 0.4442880153656006\n",
      "Loss: 0.515413761138916\n",
      "Loss: 0.5494047999382019\n",
      "Loss: 0.44601377844810486\n",
      "Loss: 0.4181988835334778\n",
      "Loss: 0.4189682900905609\n",
      "Loss: 0.36486896872520447\n",
      "Loss: 0.339600533246994\n",
      "Loss: 0.22212114930152893\n",
      "Loss: 0.34316593408584595\n",
      "Loss: 0.38351625204086304\n",
      "Loss: 0.44872984290122986\n",
      "Loss: 0.3992135226726532\n",
      "Loss: 0.4072500765323639\n",
      "Loss: 0.36685317754745483\n",
      "Loss: 0.5145268440246582\n",
      "Loss: 0.14207027852535248\n",
      "Loss: 0.37933170795440674\n",
      "Loss: 0.5164428949356079\n",
      "Loss: 0.5004847645759583\n",
      "Loss: 0.45973747968673706\n",
      "Loss: 0.18807649612426758\n",
      "Loss: 0.4297957420349121\n",
      "Loss: 0.3947994112968445\n",
      "Loss: 0.48649412393569946\n",
      "Loss: 0.2689072787761688\n",
      "Loss: 0.3693313002586365\n",
      "Loss: 0.31156665086746216\n",
      "Loss: 0.36481401324272156\n",
      "Loss: 0.5318554043769836\n",
      "Loss: 0.5905740857124329\n",
      "Loss: 0.385528564453125\n",
      "Loss: 0.5290330648422241\n",
      "Loss: 0.531417965888977\n",
      "Loss: 0.4718283712863922\n",
      "Loss: 0.38671934604644775\n",
      "Loss: 0.45002228021621704\n",
      "Loss: 0.3099430203437805\n",
      "Loss: 0.40777620673179626\n",
      "Loss: 0.4086452126502991\n",
      "Loss: 0.514589786529541\n",
      "Loss: 0.32422298192977905\n",
      "Loss: 0.6452023386955261\n",
      "Loss: 0.49070531129837036\n",
      "Loss: 0.5001574754714966\n",
      "Loss: 0.43765681982040405\n",
      "Loss: 0.32928580045700073\n",
      "Loss: 0.462697833776474\n",
      "Loss: 0.5204324722290039\n",
      "Loss: 0.34240418672561646\n",
      "Loss: 0.4988425374031067\n",
      "Loss: 0.3215988278388977\n",
      "Loss: 0.46554452180862427\n",
      "Loss: 0.44070881605148315\n",
      "Loss: 0.408383846282959\n",
      "Loss: 0.4592083990573883\n",
      "Loss: 0.31394675374031067\n",
      "Loss: 0.5541154742240906\n",
      "Loss: 0.44937586784362793\n",
      "Loss: 0.3523694574832916\n",
      "Loss: 0.39472854137420654\n",
      "Loss: 0.454606294631958\n",
      "Loss: 0.40028709173202515\n",
      "Loss: 0.5350189208984375\n",
      "Loss: 0.46607694029808044\n",
      "Loss: 0.3631685972213745\n",
      "Loss: 0.4933837354183197\n",
      "Loss: 0.3307325839996338\n",
      "Loss: 0.4593343138694763\n",
      "Loss: 0.3496462404727936\n",
      "Loss: 0.34986335039138794\n",
      "Loss: 0.46641501784324646\n",
      "Loss: 0.32508301734924316\n",
      "Loss: 0.49679121375083923\n",
      "Loss: 0.3378698527812958\n",
      "Loss: 0.36630040407180786\n",
      "Loss: 0.4677409529685974\n",
      "Loss: 0.29391658306121826\n",
      "Loss: 0.41958028078079224\n",
      "Loss: 0.3075389266014099\n",
      "Loss: 0.5390784740447998\n",
      "Loss: 0.28285542130470276\n",
      "Loss: 0.4708925783634186\n",
      "Loss: 0.44018450379371643\n",
      "Loss: 0.5169060230255127\n",
      "Loss: 0.4102610945701599\n",
      "Loss: 0.3373355269432068\n",
      "Loss: 0.4159611761569977\n",
      "Loss: 0.33151212334632874\n",
      "Loss: 0.346250057220459\n",
      "Loss: 0.5288191437721252\n",
      "Loss: 0.4358077943325043\n",
      "Loss: 0.34640228748321533\n",
      "Loss: 0.4148539900779724\n",
      "Loss: 0.3689516484737396\n",
      "Loss: 0.45760464668273926\n",
      "Loss: 0.3554559648036957\n",
      "Loss: 0.4863423705101013\n",
      "Loss: 0.3491422235965729\n",
      "Loss: 0.40052977204322815\n",
      "Loss: 0.3460754156112671\n",
      "Loss: 0.4826909005641937\n",
      "Loss: 0.37849169969558716\n",
      "Loss: 0.5400475263595581\n",
      "Loss: 0.3403034210205078\n",
      "Loss: 0.42629432678222656\n",
      "Loss: 0.30713456869125366\n",
      "Loss: 0.4763292670249939\n",
      "Loss: 0.46009311079978943\n",
      "Loss: 0.30924949049949646\n",
      "Loss: 0.44148674607276917\n",
      "Loss: 0.2583826184272766\n",
      "Loss: 0.3739343285560608\n",
      "Loss: 0.4181886613368988\n",
      "Loss: 0.4269801676273346\n",
      "Loss: 0.39576226472854614\n",
      "Loss: 0.44804173707962036\n",
      "Loss: 0.2907385230064392\n",
      "Loss: 0.39574745297431946\n",
      "Loss: 0.455725759267807\n",
      "Loss: 0.2777097225189209\n",
      "Loss: 0.5151498913764954\n",
      "Loss: 0.41661328077316284\n",
      "Loss: 0.348501980304718\n",
      "Loss: 0.3769157826900482\n",
      "Loss: 0.5176698565483093\n",
      "Loss: 0.45051223039627075\n",
      "Loss: 0.3784923851490021\n",
      "Loss: 0.4936099946498871\n",
      "Loss: 0.43682926893234253\n",
      "Loss: 0.3086439371109009\n",
      "Loss: 0.3894597589969635\n",
      "Loss: 0.3759629726409912\n",
      "Loss: 0.4537753462791443\n",
      "Loss: 0.5627163648605347\n",
      "Loss: 0.3584569990634918\n",
      "Loss: 0.4260672628879547\n",
      "Loss: 0.4612331986427307\n",
      "Loss: 0.44465455412864685\n",
      "Loss: 0.4528178572654724\n",
      "Loss: 0.45335841178894043\n",
      "Loss: 0.41544145345687866\n",
      "Loss: 0.4489441514015198\n",
      "Loss: 0.23881837725639343\n",
      "Loss: 0.5960952043533325\n",
      "Loss: 0.38565605878829956\n",
      "Loss: 0.45315712690353394\n",
      "Loss: 0.338398814201355\n",
      "Loss: 0.3478797674179077\n",
      "Loss: 0.35390424728393555\n",
      "Loss: 0.43211501836776733\n",
      "Loss: 0.3084755539894104\n",
      "Loss: 0.2755436599254608\n",
      "Loss: 0.4831387996673584\n",
      "Loss: 0.3128337562084198\n",
      "Loss: 0.3383399248123169\n",
      "Loss: 0.4124981760978699\n",
      "Loss: 0.40340203046798706\n",
      "Loss: 0.41822323203086853\n",
      "Loss: 0.42278534173965454\n",
      "Loss: 0.482201486825943\n",
      "Loss: 0.44593435525894165\n",
      "Loss: 0.5458409786224365\n",
      "Loss: 0.3398008346557617\n",
      "Loss: 0.3591092526912689\n",
      "Loss: 0.41982537508010864\n",
      "Loss: 0.4493323266506195\n",
      "Loss: 0.4913824498653412\n",
      "Loss: 0.391487181186676\n",
      "Loss: 0.5443925261497498\n",
      "Loss: 0.42823687195777893\n",
      "Loss: 0.45057111978530884\n",
      "Loss: 0.372112512588501\n",
      "Loss: 0.2825779318809509\n",
      "Loss: 0.36931976675987244\n",
      "Loss: 0.45004624128341675\n",
      "Loss: 0.4214742183685303\n",
      "Loss: 0.503388524055481\n",
      "Loss: 0.4423104226589203\n",
      "Loss: 0.5635486841201782\n",
      "Loss: 0.4022120535373688\n",
      "Loss: 0.40344709157943726\n",
      "Loss: 0.28247374296188354\n",
      "Loss: 0.30913060903549194\n",
      "Loss: 0.5620927810668945\n",
      "Loss: 0.5123376250267029\n",
      "Loss: 0.4827151298522949\n",
      "Loss: 0.4194728434085846\n",
      "Loss: 0.44769710302352905\n",
      "Loss: 0.4868183135986328\n",
      "Loss: 0.40989214181900024\n",
      "Loss: 0.5039955377578735\n",
      "Loss: 0.3393665552139282\n",
      "Loss: 0.43219810724258423\n",
      "Loss: 0.20795154571533203\n",
      "Loss: 0.38295528292655945\n",
      "Loss: 0.5156254768371582\n",
      "Loss: 0.32816043496131897\n",
      "Loss: 0.19176414608955383\n",
      "Loss: 0.4353972375392914\n",
      "Loss: 0.34584659337997437\n",
      "Loss: 0.534187912940979\n",
      "Loss: 0.3876973092556\n",
      "Loss: 0.4572621285915375\n",
      "Loss: 0.4111166000366211\n",
      "Loss: 0.44623079895973206\n",
      "Loss: 0.4083128571510315\n",
      "Loss: 0.44955429434776306\n",
      "Loss: 0.4656657576560974\n",
      "Loss: 0.44812726974487305\n",
      "Loss: 0.3919001817703247\n",
      "Loss: 0.381836861371994\n",
      "Loss: 0.5003572702407837\n",
      "Loss: 0.3223744332790375\n",
      "Loss: 0.5076695680618286\n",
      "Loss: 0.3287685513496399\n",
      "Loss: 0.45209449529647827\n",
      "Loss: 0.34542131423950195\n",
      "Loss: 0.32704851031303406\n",
      "Loss: 0.39724597334861755\n",
      "Loss: 0.33658963441848755\n",
      "Loss: 0.5312944054603577\n",
      "Loss: 0.4637583792209625\n",
      "Loss: 0.5074758529663086\n",
      "Loss: 0.40982112288475037\n",
      "Loss: 0.27406999468803406\n",
      "Loss: 0.49890583753585815\n",
      "Loss: 0.4605935215950012\n",
      "Loss: 0.4602373540401459\n",
      "Loss: 0.5123459696769714\n",
      "Loss: 0.40191221237182617\n",
      "Loss: 0.44790759682655334\n",
      "Loss: 0.3739257752895355\n",
      "Loss: 0.46578502655029297\n",
      "Loss: 0.450187623500824\n",
      "Loss: 0.31436222791671753\n",
      "Loss: 0.5457339286804199\n",
      "Loss: 0.5313793420791626\n",
      "Loss: 0.44933050870895386\n",
      "Loss: 0.5381478071212769\n",
      "Loss: 0.32118719816207886\n",
      "Loss: 0.4655560553073883\n",
      "Loss: 0.45930758118629456\n",
      "Loss: 0.34657779335975647\n",
      "Loss: 0.38584035634994507\n",
      "Loss: 0.46239790320396423\n",
      "Loss: 0.4040066599845886\n",
      "Loss: 0.38035690784454346\n",
      "Loss: 0.40649986267089844\n",
      "Loss: 0.4129388928413391\n",
      "Loss: 0.4949139654636383\n",
      "Loss: 0.2682759761810303\n",
      "Loss: 0.2725992202758789\n",
      "Loss: 0.45002785325050354\n",
      "Loss: 0.27585354447364807\n",
      "Loss: 0.4529394209384918\n",
      "Loss: 0.4162794053554535\n",
      "Loss: 0.31469517946243286\n",
      "Loss: 0.5152444243431091\n",
      "Loss: 0.4631739556789398\n",
      "Loss: 0.5194686651229858\n",
      "Loss: 0.4501689374446869\n",
      "Loss: 0.275471955537796\n",
      "Loss: 0.3319348692893982\n",
      "Loss: 0.4759922921657562\n",
      "Loss: 0.40593528747558594\n",
      "Loss: 0.3354260325431824\n",
      "Loss: 0.34043461084365845\n",
      "Loss: 0.43410342931747437\n",
      "Loss: 0.3062993288040161\n",
      "Loss: 0.35463908314704895\n",
      "Loss: 0.544080913066864\n",
      "Loss: 0.4710925221443176\n",
      "Loss: 0.41513365507125854\n",
      "Loss: 0.4054397940635681\n",
      "Loss: 0.44729918241500854\n",
      "Loss: 0.49355950951576233\n",
      "Loss: 0.3863842487335205\n",
      "Loss: 0.3257032036781311\n",
      "Loss: 0.9089052081108093\n",
      "Loss: 0.5404594540596008\n",
      "Loss: 0.4750221371650696\n",
      "Loss: 0.4537370204925537\n",
      "Loss: 0.5594413876533508\n",
      "Loss: 0.5628264546394348\n",
      "Loss: 0.40751323103904724\n",
      "Loss: 0.36263585090637207\n",
      "Loss: 0.43105387687683105\n",
      "Loss: 0.5546153783798218\n",
      "Loss: 0.21379545331001282\n",
      "Loss: 0.4488472044467926\n",
      "Loss: 0.31965044140815735\n",
      "Loss: 0.38750383257865906\n",
      "Loss: 0.3796064853668213\n",
      "Loss: 0.43817681074142456\n",
      "Loss: 0.49138370156288147\n",
      "Loss: 0.3799785077571869\n",
      "Loss: 0.44393301010131836\n",
      "Loss: 0.4317399859428406\n",
      "Loss: 0.2722821831703186\n",
      "Loss: 0.32288315892219543\n",
      "Loss: 0.3408340513706207\n",
      "Loss: 0.34636396169662476\n",
      "Loss: 0.3872341513633728\n",
      "Loss: 0.31305763125419617\n",
      "Loss: 0.46850770711898804\n",
      "Loss: 0.36361417174339294\n",
      "Loss: 0.4071771502494812\n",
      "Loss: 0.4493831992149353\n",
      "Loss: 0.3503388464450836\n",
      "Loss: 0.3541911840438843\n",
      "Loss: 0.3660386800765991\n",
      "Loss: 0.5162145495414734\n",
      "Loss: 0.488079309463501\n",
      "Loss: 0.45971494913101196\n",
      "Loss: 0.47912368178367615\n",
      "Loss: 0.4354856014251709\n",
      "Loss: 0.46400660276412964\n",
      "Loss: 0.3487533628940582\n",
      "Loss: 0.31255343556404114\n",
      "Loss: 0.44999775290489197\n",
      "Loss: 0.5304771661758423\n",
      "Loss: 0.46338462829589844\n",
      "Loss: 0.5510686039924622\n",
      "Loss: 0.42920374870300293\n",
      "Loss: 0.5492455959320068\n",
      "Loss: 0.414653480052948\n",
      "Loss: 0.4847957193851471\n",
      "Loss: 0.3523797392845154\n",
      "Loss: 0.45649999380111694\n",
      "Loss: 0.3415089249610901\n",
      "Loss: 0.4147692620754242\n",
      "Loss: 0.4447535574436188\n",
      "Loss: 0.4122966229915619\n",
      "Loss: 0.3211579918861389\n",
      "Loss: 0.37069010734558105\n",
      "Loss: 0.39241695404052734\n",
      "Loss: 0.37747836112976074\n",
      "Loss: 0.42133650183677673\n",
      "Loss: 0.4474649429321289\n",
      "Loss: 0.20982949435710907\n",
      "Loss: 0.32196155190467834\n",
      "Loss: 0.4427906572818756\n",
      "Loss: 0.5204275250434875\n",
      "Loss: 0.34167781472206116\n",
      "Loss: 0.43132084608078003\n",
      "Loss: 0.37673383951187134\n",
      "Loss: 0.3808465600013733\n",
      "Loss: 0.3792036473751068\n",
      "Loss: 0.41828665137290955\n",
      "Loss: 0.5025217533111572\n",
      "Loss: 0.4793904721736908\n",
      "Loss: 0.20531246066093445\n",
      "Loss: 0.4172130227088928\n",
      "Loss: 0.42054471373558044\n",
      "Loss: 0.31208592653274536\n",
      "Loss: 0.4199504256248474\n",
      "Loss: 0.2963795065879822\n",
      "Loss: 0.587268054485321\n",
      "Loss: 0.3784867823123932\n",
      "Loss: 0.5171915888786316\n",
      "Loss: 0.3177557587623596\n",
      "Loss: 0.5219953656196594\n",
      "Loss: 0.34930944442749023\n",
      "Loss: 0.41505637764930725\n",
      "Loss: 0.5126210451126099\n",
      "Loss: 0.30856603384017944\n",
      "Loss: 0.5659066438674927\n"
     ]
    }
   ],
   "source": [
    "epochs = 10 # number of epochs\n",
    "\n",
    "losses = []\n",
    "\n",
    "for epoch in range(1,epochs+1):\n",
    "    for batch_data, batch_labels in dataloader:\n",
    "        # Forward pass with masking\n",
    "        output = model(batch_data)\n",
    "        # print(f\"output shape: {output.shape},output: {output}\")  # Output will have shape: [batch_size, num_classes]\n",
    "        # print(f\"labels: {batch_labels}\")\n",
    "        # Example of using CrossEntropyLoss for training\n",
    "        loss = criterion(output, batch_labels)\n",
    "        print(f\"Loss: {loss.item()}\")\n",
    "        losses.append(loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
