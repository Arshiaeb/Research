{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'A_Ising'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pad\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mA_Ising\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m target_size\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'A_Ising'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import pickle\n",
    "from torch.nn.functional import pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 128, 12)]    0           []                               \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 128, 256)     3328        ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " layer_normalization (LayerNorm  (None, 128, 256)    512         ['conv1d[0][0]']                 \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " multi_head_attention (MultiHea  (None, 128, 256)    1051904     ['layer_normalization[0][0]',    \n",
      " dAttention)                                                      'layer_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 128, 256)     0           ['multi_head_attention[0][0]']   \n",
      "                                                                                                  \n",
      " tf.__operators__.add (TFOpLamb  (None, 128, 256)    0           ['dropout[0][0]',                \n",
      " da)                                                              'conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_1 (LayerNo  (None, 128, 256)    512         ['tf.__operators__.add[0][0]']   \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 128, 256)     65792       ['layer_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 128, 256)     0           ['conv1d_1[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)              (None, 128, 256)     65792       ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " tf.__operators__.add_1 (TFOpLa  (None, 128, 256)    0           ['conv1d_2[0][0]',               \n",
      " mbda)                                                            'tf.__operators__.add[0][0]']   \n",
      "                                                                                                  \n",
      " layer_normalization_2 (LayerNo  (None, 128, 256)    512         ['tf.__operators__.add_1[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_1 (MultiH  (None, 128, 256)    1051904     ['layer_normalization_2[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 128, 256)     0           ['multi_head_attention_1[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_2 (TFOpLa  (None, 128, 256)    0           ['dropout_2[0][0]',              \n",
      " mbda)                                                            'tf.__operators__.add_1[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_3 (LayerNo  (None, 128, 256)    512         ['tf.__operators__.add_2[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)              (None, 128, 256)     65792       ['layer_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 128, 256)     0           ['conv1d_3[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_4 (Conv1D)              (None, 128, 256)     65792       ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " tf.__operators__.add_3 (TFOpLa  (None, 128, 256)    0           ['conv1d_4[0][0]',               \n",
      " mbda)                                                            'tf.__operators__.add_2[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_4 (LayerNo  (None, 128, 256)    512         ['tf.__operators__.add_3[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_2 (MultiH  (None, 128, 256)    1051904     ['layer_normalization_4[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 128, 256)     0           ['multi_head_attention_2[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_4 (TFOpLa  (None, 128, 256)    0           ['dropout_4[0][0]',              \n",
      " mbda)                                                            'tf.__operators__.add_3[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_5 (LayerNo  (None, 128, 256)    512         ['tf.__operators__.add_4[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv1d_5 (Conv1D)              (None, 128, 256)     65792       ['layer_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 128, 256)     0           ['conv1d_5[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_6 (Conv1D)              (None, 128, 256)     65792       ['dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " tf.__operators__.add_5 (TFOpLa  (None, 128, 256)    0           ['conv1d_6[0][0]',               \n",
      " mbda)                                                            'tf.__operators__.add_4[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_6 (LayerNo  (None, 128, 256)    512         ['tf.__operators__.add_5[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_3 (MultiH  (None, 128, 256)    1051904     ['layer_normalization_6[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 128, 256)     0           ['multi_head_attention_3[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_6 (TFOpLa  (None, 128, 256)    0           ['dropout_6[0][0]',              \n",
      " mbda)                                                            'tf.__operators__.add_5[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_7 (LayerNo  (None, 128, 256)    512         ['tf.__operators__.add_6[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv1d_7 (Conv1D)              (None, 128, 256)     65792       ['layer_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 128, 256)     0           ['conv1d_7[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_8 (Conv1D)              (None, 128, 256)     65792       ['dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      " tf.__operators__.add_7 (TFOpLa  (None, 128, 256)    0           ['conv1d_8[0][0]',               \n",
      " mbda)                                                            'tf.__operators__.add_6[0][0]'] \n",
      "                                                                                                  \n",
      " global_average_pooling1d (Glob  (None, 256)         0           ['tf.__operators__.add_7[0][0]'] \n",
      " alAveragePooling1D)                                                                              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 128)          32896       ['global_average_pooling1d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 128)          0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 10)           1290        ['dropout_8[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,775,562\n",
      "Trainable params: 4,775,562\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the Transformer model for classification with padding and masking\n",
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, model_dim, num_heads, num_layers, num_classes, dropout=0.1):\n",
    "        super(TransformerClassifier, self).__init__()\n",
    "        self.model_dim = model_dim\n",
    "        \n",
    "        # Embedding layer to convert input to model dimensions\n",
    "        self.embedding = nn.Linear(input_dim, model_dim)\n",
    "        \n",
    "        # Positional encoding for variable length sequences\n",
    "        self.pos_encoder = PositionalEncoding(model_dim, dropout)\n",
    "        \n",
    "        # Transformer encoder\n",
    "        encoder_layers = TransformerEncoderLayer(model_dim, num_heads, model_dim, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, num_layers)\n",
    "        \n",
    "        # Classification head\n",
    "        self.fc_out = nn.Linear(model_dim, num_classes)  # Output layer for classification\n",
    "\n",
    "    def forward(self, src, src_mask):\n",
    "        src = self.embedding(src) * torch.sqrt(torch.tensor(self.model_dim, dtype=torch.float32))\n",
    "        src = self.pos_encoder(src)\n",
    "        transformer_output = self.transformer_encoder(src, src_mask)\n",
    "        \n",
    "        # Aggregate the output of the transformer (e.g., using mean pooling or just the first token)\n",
    "        pooled_output = transformer_output.mean(dim=0)\n",
    "        \n",
    "        # Pass through classification layer\n",
    "        output = self.fc_out(pooled_output)\n",
    "        return output\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, model_dim, dropout, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, model_dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, model_dim, 2).float() * (-torch.log(torch.tensor(10000.0)) / model_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "# Example usage with variable-length sequences:\n",
    "input_dim = 12  # Number of features\n",
    "model_dim = 12  # Transformer model dimension\n",
    "num_heads = 4   # Number of attention heads\n",
    "num_layers = 3  # Number of transformer layers\n",
    "num_classes = 2  # Number of classes for classification\n",
    "dropout = 0.1   # Dropout rate\n",
    "\n",
    "# Example of a batch of sequences with different lengths\n",
    "seq1 = torch.randn(8, 10)  # 8 time steps, 10 features\n",
    "seq2 = torch.randn(5, 10)  # 5 time steps, 10 features\n",
    "seq3 = torch.randn(7, 10)  # 7 time steps, 10 features\n",
    "\n",
    "# Pad the sequences to match the longest sequence length\n",
    "padded_sequences = pad_sequence([seq1, seq2, seq3], batch_first=False)  # Shape: [max_seq_len, batch_size, input_dim]\n",
    "\n",
    "# Create a mask to ignore the padded positions\n",
    "def create_mask(padded_seqs):\n",
    "    # Create a binary mask where 1 indicates valid tokens and 0 indicates padding\n",
    "    return (padded_seqs != 0).float()\n",
    "\n",
    "src_mask = create_mask(padded_sequences[:, :, 0])  # Shape: [max_seq_len, batch_size]\n",
    "\n",
    "# Initialize the transformer classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerClassifier(input_dim, model_dim, num_heads, num_layers, num_classes, dropout)\n",
    "\n",
    "# Forward pass with masking\n",
    "output = model(padded_sequences, src_mask)\n",
    "print(output.shape)  # Output will have shape: [batch_size, num_classes]\n",
    "\n",
    "# Example of using CrossEntropyLoss for training\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "labels = torch.randint(0, num_classes, (padded_sequences.size(1),))  # Random labels for testing\n",
    "loss = criterion(output, labels)\n",
    "print(f\"Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>System</th>\n",
       "      <th>System_cg</th>\n",
       "      <th>System_ews</th>\n",
       "      <th>Magnetization</th>\n",
       "      <th>Heat_capacity</th>\n",
       "      <th>Tc</th>\n",
       "      <th>Tbounds</th>\n",
       "      <th>null</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[[-1.0, 1.0, -1.0, -1.0, -0.3884, -1.0, -1.0,...</td>\n",
       "      <td>[[[-0.1378530612244898, 0.14736326530612243, -...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[0.018412698412698412, 0.018412698412698412, 0...</td>\n",
       "      <td>[0.33963944335366997, 0.3579124328155069, 0.35...</td>\n",
       "      <td>40.875444</td>\n",
       "      <td>[62.606506486380326, 52.452706542391226]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0...</td>\n",
       "      <td>[[[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[0.9922222222222222, 0.9861111111111112, 0.988...</td>\n",
       "      <td>[1.2372017981419958, 1.3653345443721225, 1.297...</td>\n",
       "      <td>24.084911</td>\n",
       "      <td>[11.616837246892846, 18.783102486882193]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[[-1.0, -0.6432, -1.0, -1.0, 0.9332, 1.0, 0.5...</td>\n",
       "      <td>[[[-0.163825, -0.138975, 0.0859, -0.0919249999...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[-0.012222222222222223, -0.0033333333333333335...</td>\n",
       "      <td>[0.31869514155341827, 0.3215510437136325, 0.32...</td>\n",
       "      <td>11.122186</td>\n",
       "      <td>[17.55984521695178, 15.251016519053056]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[[1.0, 1.0, 1.0, 0.404, 1.0, 1.0, 1.0, 1.0, 1...</td>\n",
       "      <td>[[[0.9844, 0.9337777777777778, 1.0, 1.0, 1.0, ...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[0.9792592592592593, 0.9822222222222222, 0.971...</td>\n",
       "      <td>[0.7742778242394478, 0.7550195841953804, 0.859...</td>\n",
       "      <td>3.977646</td>\n",
       "      <td>[2.5002202103401916, 3.977646440808179]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0...</td>\n",
       "      <td>[[[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[0.9866666666666667, 0.9833333333333333, 0.983...</td>\n",
       "      <td>[0.3144528813989275, 0.3613046362387088, 0.374...</td>\n",
       "      <td>5.388354</td>\n",
       "      <td>[3.5310727194934532, 2.3656714311515787]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[[[-1.0, 0.7744, 1.0, 0.9052, -1.0, -1.0, -1.0...</td>\n",
       "      <td>[[[0.019600000000000006, -0.004777777777777757...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[0.02074074074074074, 0.007901234567901235, 0....</td>\n",
       "      <td>[0.7464312119787391, 0.7356450914805108, 0.747...</td>\n",
       "      <td>24.306963</td>\n",
       "      <td>[37.41898618920932, 24.30696310131024]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[[[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0...</td>\n",
       "      <td>[[[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99115102040...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[0.9912925170068028, 0.9847619047619047, 0.981...</td>\n",
       "      <td>[0.5813449320773951, 0.6639666250178324, 0.700...</td>\n",
       "      <td>7.069180</td>\n",
       "      <td>[4.581194000157167, 3.9339484262111943]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[[[0.4412, 1.0, 1.0, 0.4676, 0.1076, 0.5796, -...</td>\n",
       "      <td>[[[0.2405, 0.1734, 0.4024111111111111, -0.1209...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[-0.018271604938271607, 0.008148148148148147, ...</td>\n",
       "      <td>[0.49213712640097385, 0.5003961170853308, 0.50...</td>\n",
       "      <td>4.797242</td>\n",
       "      <td>[6.274745527578487, 7.580653707229478]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[[[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0...</td>\n",
       "      <td>[[[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[0.9992743764172336, 0.999092970521542, 0.9989...</td>\n",
       "      <td>[0.3901518276599921, 0.3970269147229335, 0.402...</td>\n",
       "      <td>9.386998</td>\n",
       "      <td>[4.361125586955422, 6.772710006220633]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[[[-0.1364, 0.2336, -1.0, -1.0, -1.0, -0.3244,...</td>\n",
       "      <td>[[[0.0974222222222222, 0.0919555555555556, -0....</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[-0.05185185185185185, -0.0854320987654321, -0...</td>\n",
       "      <td>[0.41895332146957903, 0.46079408881288497, 0.4...</td>\n",
       "      <td>5.738842</td>\n",
       "      <td>[8.206871856128657, 8.014981605810522]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              System  \\\n",
       "0  [[[-1.0, 1.0, -1.0, -1.0, -0.3884, -1.0, -1.0,...   \n",
       "1  [[[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0...   \n",
       "2  [[[-1.0, -0.6432, -1.0, -1.0, 0.9332, 1.0, 0.5...   \n",
       "3  [[[1.0, 1.0, 1.0, 0.404, 1.0, 1.0, 1.0, 1.0, 1...   \n",
       "4  [[[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0...   \n",
       "5  [[[-1.0, 0.7744, 1.0, 0.9052, -1.0, -1.0, -1.0...   \n",
       "6  [[[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0...   \n",
       "7  [[[0.4412, 1.0, 1.0, 0.4676, 0.1076, 0.5796, -...   \n",
       "8  [[[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0...   \n",
       "9  [[[-0.1364, 0.2336, -1.0, -1.0, -1.0, -0.3244,...   \n",
       "\n",
       "                                           System_cg  \\\n",
       "0  [[[-0.1378530612244898, 0.14736326530612243, -...   \n",
       "1  [[[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0...   \n",
       "2  [[[-0.163825, -0.138975, 0.0859, -0.0919249999...   \n",
       "3  [[[0.9844, 0.9337777777777778, 1.0, 1.0, 1.0, ...   \n",
       "4  [[[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0...   \n",
       "5  [[[0.019600000000000006, -0.004777777777777757...   \n",
       "6  [[[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99115102040...   \n",
       "7  [[[0.2405, 0.1734, 0.4024111111111111, -0.1209...   \n",
       "8  [[[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0...   \n",
       "9  [[[0.0974222222222222, 0.0919555555555556, -0....   \n",
       "\n",
       "                                          System_ews  \\\n",
       "0  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "1  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "2  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "3  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "4  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "5  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "6  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "7  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "8  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "9  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "\n",
       "                                       Magnetization  \\\n",
       "0  [0.018412698412698412, 0.018412698412698412, 0...   \n",
       "1  [0.9922222222222222, 0.9861111111111112, 0.988...   \n",
       "2  [-0.012222222222222223, -0.0033333333333333335...   \n",
       "3  [0.9792592592592593, 0.9822222222222222, 0.971...   \n",
       "4  [0.9866666666666667, 0.9833333333333333, 0.983...   \n",
       "5  [0.02074074074074074, 0.007901234567901235, 0....   \n",
       "6  [0.9912925170068028, 0.9847619047619047, 0.981...   \n",
       "7  [-0.018271604938271607, 0.008148148148148147, ...   \n",
       "8  [0.9992743764172336, 0.999092970521542, 0.9989...   \n",
       "9  [-0.05185185185185185, -0.0854320987654321, -0...   \n",
       "\n",
       "                                       Heat_capacity         Tc  \\\n",
       "0  [0.33963944335366997, 0.3579124328155069, 0.35...  40.875444   \n",
       "1  [1.2372017981419958, 1.3653345443721225, 1.297...  24.084911   \n",
       "2  [0.31869514155341827, 0.3215510437136325, 0.32...  11.122186   \n",
       "3  [0.7742778242394478, 0.7550195841953804, 0.859...   3.977646   \n",
       "4  [0.3144528813989275, 0.3613046362387088, 0.374...   5.388354   \n",
       "5  [0.7464312119787391, 0.7356450914805108, 0.747...  24.306963   \n",
       "6  [0.5813449320773951, 0.6639666250178324, 0.700...   7.069180   \n",
       "7  [0.49213712640097385, 0.5003961170853308, 0.50...   4.797242   \n",
       "8  [0.3901518276599921, 0.3970269147229335, 0.402...   9.386998   \n",
       "9  [0.41895332146957903, 0.46079408881288497, 0.4...   5.738842   \n",
       "\n",
       "                                    Tbounds  null  \n",
       "0  [62.606506486380326, 52.452706542391226]     1  \n",
       "1  [11.616837246892846, 18.783102486882193]     1  \n",
       "2   [17.55984521695178, 15.251016519053056]     1  \n",
       "3   [2.5002202103401916, 3.977646440808179]     0  \n",
       "4  [3.5310727194934532, 2.3656714311515787]     1  \n",
       "5    [37.41898618920932, 24.30696310131024]     0  \n",
       "6   [4.581194000157167, 3.9339484262111943]     1  \n",
       "7    [6.274745527578487, 7.580653707229478]     1  \n",
       "8    [4.361125586955422, 6.772710006220633]     1  \n",
       "9    [8.206871856128657, 8.014981605810522]     1  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_size = 15\n",
    "with open(\"Train_Data_15\", 'rb') as file:\n",
    "    raw_data = pickle.load(file)\n",
    "raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(549, 12) <class 'numpy.ndarray'>\n",
      "(574, 12) <class 'numpy.ndarray'>\n",
      "(573, 12) <class 'numpy.ndarray'>\n",
      "(580, 12) <class 'numpy.ndarray'>\n",
      "(570, 12) <class 'numpy.ndarray'>\n",
      "(50, 12) <class 'numpy.ndarray'>\n",
      "(550, 12) <class 'numpy.ndarray'>\n",
      "(533, 12) <class 'numpy.ndarray'>\n",
      "(530, 12) <class 'numpy.ndarray'>\n",
      "(475, 12) <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "for thing in raw_data[\"System_ews\"]:\n",
    "    print(thing.shape,type(thing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50, 12), (50, 15, 15))"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data[\"System_ews\"][5].shape, raw_data[\"System_cg\"][5].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq = [torch.from_numpy(run) for run in raw_data[\"System_ews\"]]\n",
    "torch.equal(seq[3][0],torch.zeros(12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = [torch.from_numpy(run) for run in raw_data[\"System_ews\"]]\n",
    "seq_trunc = []\n",
    "for run in seq:\n",
    "    t = 0\n",
    "    try:\n",
    "        while torch.equal(run[t],torch.zeros(12)):\n",
    "            t += 1\n",
    "        seq_trunc.append(run[t:,:])\n",
    "    except IndexError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([438, 12]) <class 'torch.Tensor'>\n",
      "torch.Size([445, 12]) <class 'torch.Tensor'>\n",
      "torch.Size([435, 12]) <class 'torch.Tensor'>\n",
      "torch.Size([408, 12]) <class 'torch.Tensor'>\n",
      "torch.Size([460, 12]) <class 'torch.Tensor'>\n",
      "torch.Size([408, 12]) <class 'torch.Tensor'>\n",
      "torch.Size([399, 12]) <class 'torch.Tensor'>\n",
      "torch.Size([422, 12]) <class 'torch.Tensor'>\n",
      "torch.Size([285, 12]) <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "for thing in seq_trunc:\n",
    "    print(thing.shape,type(thing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.equal(seq[3][0],torch.zeros(target_size,target_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "580"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length = max ([thing.shape[0] for thing in raw_data[\"System_cg\"]])\n",
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = [torch.from_numpy(thing) for thing in raw_data[\"System_cg\"]]\n",
    "news = []\n",
    "for thing in seq:\n",
    "    pad_amount = max_length - thing.shape[0]\n",
    "    new_t = pad(thing, (0, 0, 0, 0, pad_amount, 0))\n",
    "    news.append(new_t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([580, 15, 15]) <class 'torch.Tensor'>\n",
      "torch.Size([580, 15, 15]) <class 'torch.Tensor'>\n",
      "torch.Size([580, 15, 15]) <class 'torch.Tensor'>\n",
      "torch.Size([580, 15, 15]) <class 'torch.Tensor'>\n",
      "torch.Size([580, 15, 15]) <class 'torch.Tensor'>\n",
      "torch.Size([580, 15, 15]) <class 'torch.Tensor'>\n",
      "torch.Size([580, 15, 15]) <class 'torch.Tensor'>\n",
      "torch.Size([580, 15, 15]) <class 'torch.Tensor'>\n",
      "torch.Size([580, 15, 15]) <class 'torch.Tensor'>\n",
      "torch.Size([580, 15, 15]) <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "for thing in news:\n",
    "    print(thing.shape,type(thing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([580, 15, 15])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_size = 15\n",
    "oner = news[0]\n",
    "torch.equal(oner[300],torch.zeros(target_size,target_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([580, 15, 15])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oner.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
